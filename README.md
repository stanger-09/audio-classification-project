# ASiT: Localâ€“Global Audio Spectrogram Vision Transformer  
### **Automated Speaker Identification Using Audio Spectrogram Transformers**

ASiT is an advanced deep learning system designed for **speaker identification** using audio spectrograms.  
It implements:

- **Global Spectrogram Masking (GMML)**
- **Localâ€“Global Feature Fusion**
- **Self-Supervised Representation Learning**
- **Transformer-based Audio Encoder**

Traditionally, speaker identification teams spent **weeks** analyzing audio data manually.  
ASiT reduces this to **seconds**, offering fast, accurate, scalable predictions.

---

## ðŸ”¥ Key Features

- **Vision Transformer (ViT)-inspired audio encoder**
- **Localâ€“Global spectrogram masking strategy**
- **Fully modular codebase**
- **High accuracy speaker prediction**
- **Efficient PyTorch training & inference pipeline**
- **Self-contained preprocessing utilities**
- **Production-ready architecture**

---

## ðŸ“‚ Project Structure
```
ASiT/
â”‚
â”œâ”€â”€ api/                         # Flask backend (deployment / demo)
â”‚   â”œâ”€â”€ app.py                   # Flask app entry point
â”‚   â”œâ”€â”€ requirements.txt         # API dependencies
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ index.html           # Frontend UI for audio upload
â”‚
â”œâ”€â”€ src/                         # Core ML codebase
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                  # Model architectures
â”‚   â”‚   â”œâ”€â”€ wav2vec_classifier.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Wav2VecClassifier
â”‚   â”‚   â”‚   â””â”€â”€ Attention pooling + classifier head
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ __pycache__/
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                # Training pipeline
â”‚   â”‚   â”œâ”€â”€ train_epoch.py       # One epoch training logic
â”‚   â”‚   â”œâ”€â”€ train_full.py        # Full training loop
â”‚   â”‚   â”œâ”€â”€ collate_fn.py        # Padding & batch handling
â”‚   â”‚   â”œâ”€â”€ accuracy.py          # Accuracy calculation
â”‚   â”‚   â””â”€â”€ __pycache__/
â”‚   â”‚
â”‚   â”œâ”€â”€ inference/               # Inference pipeline
â”‚   â”‚   â”œâ”€â”€ preprocess_audio.py  # Resampling, padding, normalization
â”‚   â”‚   â”œâ”€â”€ predict.py           # Model inference logic
â”‚   â”‚   â””â”€â”€ __pycache__/
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                   # Helper utilities
â”‚   â”‚   â”œâ”€â”€ dataset.py           # Custom Dataset class
â”‚   â”‚   â”œâ”€â”€ unzip_data.py        # Dataset extraction
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ main_train.py             # Training entry point
â”‚   â””â”€â”€ inference_main.py         # Standalone inference runner
â”‚
â”œâ”€â”€ checkpoints/                 # Saved models
â”‚   â”œâ”€â”€ best_wav2vec_classifier.pt
â”‚   â””â”€â”€ best_wav2vec_22class_classifier.pt
â”‚
â”œâ”€â”€ data/                        # (Optional local data)
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ notebooks/                   # Experiments (optional)
â”‚   â””â”€â”€ analysis.ipynb
â”‚
â”œâ”€â”€ requirements.txt             # Project-wide dependencies
â”œâ”€â”€ README.md                    # Project documentation
â””â”€â”€ .gitignore                   # Ignore cache, data, checkpoints

```
