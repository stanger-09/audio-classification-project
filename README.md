# ğŸ§ ASiT â€“ Audio Event Classification using Wav2Vec2

ASiT is an end-to-end audio event classification system built using a pretrained Wav2Vec2 model with an attention-based temporal pooling mechanism.  
The model learns directly from raw audio waveforms without relying on handcrafted acoustic features.

---

## ğŸš€ Project Highlights

- Raw waveform-based learning  
- Pretrained Wav2Vec2 backbone  
- Attention-based temporal pooling  
- 22-class audio event classification  
- Flask API for real-time inference  
- Windows & Google Colab compatible  
- Modular and research-friendly architecture  

---

## ğŸ§© Problem Statement

Traditional audio classification systems depend on handcrafted features such as MFCCs or spectrograms, which may discard important temporal information.

This project overcomes those limitations by:
- Learning representations directly from raw audio
- Leveraging self-supervised pretrained models
- Using attention pooling to focus on informative audio segments

---

## ğŸ—ï¸ Model Architecture

### Pipeline Overview

```
Audio Waveform (16 kHz)
â†“
Wav2Vec2 Feature Encoder
â†“
Frame-Level Representations
â†“
Attention-Based Temporal Pooling
â†“
Utterance-Level Embedding
â†“
Linear Classifier + Softmax
â†“
Predicted Audio Class

```

### Key Components

- **Input**: Raw audio waveform (5 seconds, 16 kHz)
- **Backbone**: `facebook/wav2vec2-base`
- **Pooling**: Learnable attention pooling layer
- **Classifier**: Fully connected layer

---

## ğŸ§ª Dataset

- Dataset Source: Generic Audio Samples (Kaggle)
- Categories:
  - Animals
  - Birds
  - Vehicles
  - Environmental sounds
- Total classes after preprocessing: **22**

---

## ğŸ“‚ Project Structure
```
ASiT/
â”œâ”€â”€ api/ # Flask backend
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ models/ # Model architecture
â”‚ â”œâ”€â”€ training/ # Training pipeline
â”‚ â”œâ”€â”€ inference/ # Inference pipeline
â”‚ â”œâ”€â”€ utils/ # Dataset utilities
â”‚ â”œâ”€â”€ main_train.py
â”‚ â””â”€â”€ inference_main.py
â”œâ”€â”€ checkpoints/ # Saved model weights
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
ğŸ“Š Results

The model learns high-level semantic representations of audio events.
Attention pooling improves robustness for variable-length audio and noisy conditions.

Performance varies based on dataset split and training configuration.

##ğŸ§  Key Concepts Used

-Wav2Vec2

-Self-Supervised Learning

-Attention-Based Pooling

-End-to-End Audio Classification

-Transfer Learning

##âš ï¸ Limitations

-Performance depends on dataset quality

-Limited data augmentation

-Single-head attention pooling

-No explicit noise-robust training

##ğŸ”® Future Improvements

-Multi-head attention pooling

-Advanced audio data augmentation

-Larger pretrained backbones

-Audio-visual multimodal learning

Dockerized deployment
